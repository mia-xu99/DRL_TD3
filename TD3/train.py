import os
import time
import numpy as np
import torch
import torch.nn as nn
import torch.nn.functional as F
from numpy import inf
from torch.utils.tensorboard import SummaryWriter

# å¯¼å…¥è‡ªå®šä¹‰æ¨¡å—
from replay_buffer import ReplayBuffer
from velodyne_env import GazeboEnv

# ==========================================
# å·¥å…·å‡½æ•°ï¼šæ•°æ®å½’ä¸€åŒ– (é¢„å¤„ç†)
# ==========================================
def process_state(state):
    """
    çŠ¶æ€é¢„å¤„ç†å‡½æ•°ã€‚
    ä½œç”¨ï¼šå°†åŸå§‹ç¯å¢ƒçŠ¶æ€è¿›è¡Œå½’ä¸€åŒ–å¤„ç†ï¼Œé˜²æ­¢æ•°å€¼è¿‡å¤§å¯¼è‡´ç¥ç»ç½‘ç»œæ¢¯åº¦çˆ†ç‚¸æˆ–éš¾ä»¥æ”¶æ•›ã€‚
    
    è¾“å…¥ state ç»“æ„å‡è®¾: [20ä¸ªé›·è¾¾å°„çº¿è·ç¦», ç›®æ ‡è·ç¦», ç›®æ ‡è§’åº¦, çº¿é€Ÿåº¦, è§’é€Ÿåº¦]
    """
    # 1. å¤„ç†é›·è¾¾æ•°æ® (å‰20ç»´)
    # å‡è®¾é›·è¾¾æœ€å¤§æ¢æµ‹è·ç¦»ä¸º 10ç±³
    lidar = state[:-4]
    lidar = np.clip(lidar, 0, 10) # æˆªæ–­æ•°æ®ï¼Œé™åˆ¶åœ¨ 0-10 ä¹‹é—´
    lidar /= 10.0                 # å½’ä¸€åŒ–åˆ° 0-1 ä¹‹é—´

    # 2. å¤„ç†æœºå™¨äººçŠ¶æ€ä¿¡æ¯ (å4ç»´)
    robot_info = state[-4:]
    robot_info[0] /= 10.0         # ç›®æ ‡è·ç¦»å½’ä¸€åŒ– (å‡è®¾æœ€å¤§å…³æ³¨è·ç¦»ä¸º10ç±³)
    # robot_info[1] æ˜¯è§’åº¦ (-pi åˆ° pi)ï¼Œé€šå¸¸æ•°å€¼èŒƒå›´å°šå¯ï¼Œå¯ä¸å¤„ç†æˆ–é™¤ä»¥ pi
    # robot_info[2], [3] æ˜¯çº¿é€Ÿåº¦å’Œè§’é€Ÿåº¦ï¼Œæ•°å€¼è¾ƒå°ï¼Œé€šå¸¸ä¿æŒåŸæ ·

    # é‡æ–°æ‹¼æ¥å½’ä¸€åŒ–åçš„æ•°æ®
    return np.concatenate((lidar, robot_info))

# ==========================================
# è¯„ä¼°å‡½æ•° (Testing/Validation)
# ==========================================
def evaluate(network, epoch, eval_episodes=10):
    """
    åœ¨ä¸æ·»åŠ æ¢ç´¢å™ªå£°çš„æƒ…å†µä¸‹è¯„ä¼°å½“å‰ç­–ç•¥çš„è¡¨ç°ã€‚
    """
    avg_reward = 0.0
    col = 0 # ç¢°æ’è®¡æ•°
    
    for _ in range(eval_episodes):
        count = 0
        raw_state = env.reset()
        state = process_state(raw_state) # è®°å¾—è¯„ä¼°æ—¶ä¹Ÿè¦å½’ä¸€åŒ–
        done = False
        episode_collision = False 

        while not done and count < 501:
            # è·å–åŠ¨ä½œ (æµ‹è¯•æ¨¡å¼ä¸‹ä¸åŠ å™ªå£°)
            action = network.get_action(np.array(state))
            # å°†è¾“å‡ºåŠ¨ä½œæ˜ å°„å›ç¯å¢ƒæ‰€éœ€èŒƒå›´
            # action[0] (çº¿é€Ÿåº¦): ç½‘ç»œè¾“å‡º [-1, 1] -> æ˜ å°„åˆ° [0, 1]
            # action[1] (è§’é€Ÿåº¦): ç½‘ç»œè¾“å‡º [-1, 1] -> ä¿æŒ [-1, 1]
            a_in = [(action[0] + 1) / 2, action[1]]
            
            raw_state, reward, done, _ = env.step(a_in)
            state = process_state(raw_state)
            avg_reward += reward
            count += 1
            
            # --- ç¢°æ’æ£€æµ‹é€»è¾‘ ---
            # å‡è®¾ç¯å¢ƒè®¾å®š reward < -90 è¡¨ç¤ºå‘ç”Ÿäº†ä¸¥é‡ç¢°æ’
            if reward < -90:
                episode_collision = True
        
        # ç»Ÿè®¡å‘ç”Ÿç¢°æ’çš„å›åˆæ•°
        if episode_collision:
            col += 1

    avg_reward /= eval_episodes
    avg_col = col / eval_episodes # è®¡ç®—ç¢°æ’ç‡
    
    print("..............................................")
    print(
        "Average Reward over %i Evaluation Episodes, Epoch %i: %f, Collision Rate: %f"
        % (eval_episodes, epoch, avg_reward, avg_col)
    )
    print("..............................................")
    return avg_reward

# ==========================================
# ç½‘ç»œå®šä¹‰ (Actor å’Œ Critic)
# ==========================================
class Actor(nn.Module):
    """
    ç­–ç•¥ç½‘ç»œ (Policy Network)
    è¾“å…¥: çŠ¶æ€ (state)
    è¾“å‡º: åŠ¨ä½œ (action) - è¿ç»­å€¼
    """
    def __init__(self, state_dim, action_dim):
        super(Actor, self).__init__()

        # ç½‘ç»œå±‚å®šä¹‰ (å·²ä¼˜åŒ–ç½‘ç»œè§„æ¨¡)
        self.layer_1 = nn.Linear(state_dim, 400)
        # Kaiming åˆå§‹åŒ–æœ‰åŠ©äºæ·±å±‚ç½‘ç»œæ”¶æ•›
        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity="leaky_relu")
        
        self.layer_2 = nn.Linear(400, 300)
        torch.nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity="leaky_relu")
        
        self.layer_3 = nn.Linear(300, action_dim)
        self.tanh = nn.Tanh() # è¾“å‡ºå±‚ä½¿ç”¨ Tanh å°†åŠ¨ä½œé™åˆ¶åœ¨ [-1, 1]

    def forward(self, s):
        s = F.leaky_relu(self.layer_1(s))
        s = F.leaky_relu(self.layer_2(s))
        a = self.tanh(self.layer_3(s))
        return a

class Critic(nn.Module):
    """
    ä»·å€¼ç½‘ç»œ (Value Network) - Twin Critic ç»“æ„
    è¾“å…¥: çŠ¶æ€ (state) + åŠ¨ä½œ (action)
    è¾“å‡º: Qå€¼ (Q-value)
    TD3 ä½¿ç”¨ä¸¤ä¸ª Critic (Q1, Q2) æ¥ç¼“è§£è¿‡é«˜ä¼°è®¡ (Overestimation) é—®é¢˜ã€‚
    """
    def __init__(self, state_dim, action_dim):
        super(Critic, self).__init__()

        # --- Q1 ç½‘ç»œæ¶æ„ ---
        self.layer_1 = nn.Linear(state_dim + action_dim, 400) # è¾“å…¥æ˜¯ state å’Œ action çš„æ‹¼æ¥
        torch.nn.init.kaiming_uniform_(self.layer_1.weight, nonlinearity="leaky_relu")
        
        self.layer_2 = nn.Linear(400, 300)
        torch.nn.init.kaiming_uniform_(self.layer_2.weight, nonlinearity="leaky_relu")
        
        self.layer_3 = nn.Linear(300, 1) # è¾“å‡ºå•ä¸ª Q å€¼

        # --- Q2 ç½‘ç»œæ¶æ„ (ç»“æ„ç›¸åŒï¼Œå‚æ•°ç‹¬ç«‹) ---
        self.layer_4 = nn.Linear(state_dim + action_dim, 400)
        torch.nn.init.kaiming_uniform_(self.layer_4.weight, nonlinearity="leaky_relu")
        
        self.layer_5 = nn.Linear(400, 300)
        torch.nn.init.kaiming_uniform_(self.layer_5.weight, nonlinearity="leaky_relu")
        
        self.layer_6 = nn.Linear(300, 1)

    def forward(self, s, a):
        sa = torch.cat([s, a], 1) # æ‹¼æ¥çŠ¶æ€å’ŒåŠ¨ä½œ

        # è®¡ç®— Q1
        q1 = F.leaky_relu(self.layer_1(sa))
        q1 = F.leaky_relu(self.layer_2(q1))
        q1 = self.layer_3(q1)

        # è®¡ç®— Q2
        q2 = F.leaky_relu(self.layer_4(sa))
        q2 = F.leaky_relu(self.layer_5(q2))
        q2 = self.layer_6(q2)
        return q1, q2

# ==========================================
# TD3 ç®—æ³•æ ¸å¿ƒé€»è¾‘
# ==========================================
class TD3(object):
    def __init__(self, state_dim, action_dim, max_action):
        lr = 1e-4 # å­¦ä¹ ç‡
        
        # åˆå§‹åŒ– Actor åŠå…¶ç›®æ ‡ç½‘ç»œ
        self.actor = Actor(state_dim, action_dim).to(device)
        self.actor_target = Actor(state_dim, action_dim).to(device)
        self.actor_target.load_state_dict(self.actor.state_dict())
        self.actor_optimizer = torch.optim.Adam(self.actor.parameters(), lr=lr)

        # åˆå§‹åŒ– Critic åŠå…¶ç›®æ ‡ç½‘ç»œ
        self.critic = Critic(state_dim, action_dim).to(device)
        self.critic_target = Critic(state_dim, action_dim).to(device)
        self.critic_target.load_state_dict(self.critic.state_dict())
        self.critic_optimizer = torch.optim.Adam(self.critic.parameters(), lr=lr)

        self.max_action = max_action
        self.writer = SummaryWriter() # ç”¨äº Tensorboard è®°å½•
        self.iter_count = 0

    def get_action(self, state):
        """å‰å‘ä¼ æ’­è·å–åŠ¨ä½œï¼Œç”¨äºäº¤äº’"""
        state = torch.tensor(state.reshape(1, -1), dtype=torch.float32).to(device)
        with torch.no_grad():
            return self.actor(state).cpu().detach().numpy().flatten()

    def train(
        self,
        replay_buffer,
        iterations,
        batch_size=40,
        discount=0.99,   # æŠ˜æ‰£å› å­ gamma
        tau=0.005,       # è½¯æ›´æ–°ç³»æ•°
        policy_noise=0.2,# ç›®æ ‡åŠ¨ä½œå¹³æ»‘å™ªå£°æ ‡å‡†å·®
        noise_clip=0.5,  # å™ªå£°è£å‰ªèŒƒå›´
        policy_freq=2,   # ç­–ç•¥æ›´æ–°é¢‘ç‡ (å»¶è¿Ÿæ›´æ–°)
    ):
        av_Q = 0
        max_Q = -inf
        av_loss = 0
        
        for it in range(iterations):
            # 1. ä»ç»éªŒå›æ”¾æ± é‡‡æ ·
            (
                batch_states,
                batch_actions,
                batch_rewards,
                batch_dones,
                batch_next_states,
            ) = replay_buffer.sample_batch(batch_size)
            
            # è½¬æ¢ä¸º Tensor å¹¶æ”¾å…¥ GPU/CPU
            state = torch.tensor(batch_states, dtype=torch.float32).to(device)
            next_state = torch.tensor(batch_next_states, dtype=torch.float32).to(device)
            action = torch.tensor(batch_actions, dtype=torch.float32).to(device)
            reward = torch.tensor(batch_rewards, dtype=torch.float32).to(device)
            done = torch.tensor(batch_dones, dtype=torch.float32).to(device)

            # 2. è®¡ç®—ç›®æ ‡ Q å€¼ (Target Q)
            with torch.no_grad():
                # ç›®æ ‡ç­–ç•¥å¹³æ»‘ (Target Policy Smoothing):
                # åœ¨ç›®æ ‡åŠ¨ä½œä¸Šæ·»åŠ å™ªå£°ï¼Œä½¿ Value ä¼°è®¡æ›´å¹³æ»‘ï¼Œé˜²æ­¢è¿‡æ‹Ÿåˆåˆ°å°–å³°
                next_action = self.actor_target(next_state)
                noise = torch.randn_like(action).normal_(0, policy_noise).to(device)
                noise = noise.clamp(-noise_clip, noise_clip)
                next_action = (next_action + noise).clamp(-self.max_action, self.max_action)

                # è·å–ä¸¤ä¸ªç›®æ ‡ Critic çš„ Q å€¼
                target_Q1, target_Q2 = self.critic_target(next_state, next_action)
                # å–æœ€å°å€¼ (Clipped Double Q-learning)ï¼Œç¼“è§£è¿‡ä¼°è®¡
                target_Q = torch.min(target_Q1, target_Q2)
                
                # è®°å½•æ•°æ®ç”¨äºåˆ†æ
                av_Q += torch.mean(target_Q)
                max_Q = max(max_Q, torch.max(target_Q).item())
                
                # Bellman æ–¹ç¨‹è®¡ç®—ç›®æ ‡å€¼
                target_Q = reward + ((1 - done) * discount * target_Q)

            # 3. è®¡ç®—å½“å‰ Q å€¼å¹¶æ›´æ–° Critic
            current_Q1, current_Q2 = self.critic(state, action)

            # ä½¿ç”¨ SmoothL1Loss (Huber Loss) ç›¸æ¯” MSE å¯¹å¼‚å¸¸å€¼æ›´ä¸æ•æ„Ÿ
            loss = F.smooth_l1_loss(current_Q1, target_Q) + F.smooth_l1_loss(current_Q2, target_Q)

            self.critic_optimizer.zero_grad()
            loss.backward()
            # æ¢¯åº¦è£å‰ª (é˜²æ­¢æ¢¯åº¦çˆ†ç‚¸)
            torch.nn.utils.clip_grad_norm_(self.critic.parameters(), max_norm=1.0) 
            self.critic_optimizer.step()

            # 4. å»¶è¿Ÿæ›´æ–° Actor (Delayed Policy Update)
            # åªæœ‰ Critic æ›´æ–° policy_freq æ¬¡åï¼Œæ‰æ›´æ–°ä¸€æ¬¡ Actor
            if it % policy_freq == 0:
                # è®¡ç®— Actor æŸå¤±: æœ€å¤§åŒ– Q1 å€¼ -> æœ€å°åŒ– -Q1
                actor_grad, _ = self.critic(state, self.actor(state))
                actor_grad = -actor_grad.mean()
                
                self.actor_optimizer.zero_grad()
                actor_grad.backward()
                self.actor_optimizer.step()

                # 5. è½¯æ›´æ–°ç›®æ ‡ç½‘ç»œ (Soft Update)
                # target_param = tau * param + (1 - tau) * target_param
                for param, target_param in zip(self.actor.parameters(), self.actor_target.parameters()):
                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)
                for param, target_param in zip(self.critic.parameters(), self.critic_target.parameters()):
                    target_param.data.copy_(tau * param.data + (1 - tau) * target_param.data)

            av_loss += loss.item()
            
        self.iter_count += 1
        # å†™å…¥ Tensorboard
        self.writer.add_scalar("loss", av_loss / iterations, self.iter_count)
        self.writer.add_scalar("Av. Q", av_Q / iterations, self.iter_count)
        self.writer.add_scalar("Max. Q", max_Q, self.iter_count)

    def save(self, filename, directory):
        torch.save(self.actor.state_dict(), "%s/%s_actor.pth" % (directory, filename))
        torch.save(self.critic.state_dict(), "%s/%s_critic.pth" % (directory, filename))

    def load(self, filename, directory):
        self.actor.load_state_dict(torch.load("%s/%s_actor.pth" % (directory, filename), map_location=device))
        self.critic.load_state_dict(torch.load("%s/%s_critic.pth" % (directory, filename), map_location=device))

# ==========================================
# ä¸»ç¨‹åºè®¾ç½®
# ==========================================
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
seed = 0
eval_freq = 5e3       # æ¯ 5000 æ­¥è¯„ä¼°ä¸€æ¬¡
max_ep = 500          # æ¯ä¸ª Episode æœ€å¤§æ­¥æ•°
eval_ep = 10          # æ¯æ¬¡è¯„ä¼°è·‘å‡ ä¸ª Episode
max_timesteps = 5e6   # æ€»è®­ç»ƒæ­¥æ•°
expl_noise = 1        # åˆå§‹æ¢ç´¢å™ªå£°
expl_decay_steps = 500000 # å™ªå£°è¡°å‡æ­¥æ•°
expl_min = 0.1        # æœ€å°æ¢ç´¢å™ªå£°
batch_size = 40       # è®­ç»ƒæ‰¹æ¬¡å¤§å°
discount = 0.99       # æŠ˜æ‰£å› å­
tau = 0.005           # è½¯æ›´æ–°å‚æ•°
policy_noise = 0.2    # ç­–ç•¥å¹³æ»‘å™ªå£°
noise_clip = 0.5      # å™ªå£°è£å‰ª
policy_freq = 2       # Actor æ›´æ–°å»¶è¿Ÿé¢‘ç‡
buffer_size = int(1e6)# ç»éªŒå›æ”¾æ± å¤§å°

# æ¨¡å‹ä¿å­˜åç§°é…ç½®
file_name = "TD3_velodyne_best"
save_model = True
load_model = False    # æ˜¯å¦åŠ è½½å·²æœ‰æ¨¡å‹
random_near_obstacle = True # æ˜¯å¦å¯ç”¨é‡éšœéšæœºç­–ç•¥
SAFE_DIST = 0.6       # å®‰å…¨è·ç¦» (ç±³)
LIDAR_MAX = 10.0      # é›·è¾¾æœ€å¤§è·ç¦»

# åˆ›å»ºç›®å½•
if not os.path.exists("./results"):
    os.makedirs("./results")
if save_model and not os.path.exists("./pytorch_models"):
    os.makedirs("./pytorch_models")

environment_dim = 20 # é›·è¾¾ç»´åº¦
robot_dim = 4        # æœºå™¨äººçŠ¶æ€ç»´åº¦
# åˆå§‹åŒ–ç¯å¢ƒ
env = GazeboEnv("multi_robot_scenario.launch", environment_dim)
print("Waiting for Gazebo and ROS nodes to fully initialize...")
time.sleep(10)

# è®¾ç½®éšæœºç§å­
torch.manual_seed(seed)
np.random.seed(seed)
state_dim = environment_dim + robot_dim
action_dim = 2
max_action = 1

# åˆå§‹åŒ– TD3 ç½‘ç»œ
network = TD3(state_dim, action_dim, max_action)
replay_buffer = ReplayBuffer(buffer_size, seed)

# åŠ è½½æ¨¡å‹ (å¦‚æœå¯ç”¨)
if load_model:
    try:
        network.load(file_name, "./pytorch_models")
    except Exception as e:
        print(f"Could not load model: {e}")

evaluations = []
timestep = 0
timesteps_since_eval = 0
episode_num = 0
done = True
epoch = 1

count_rand_actions = 0
random_action = []

# åˆå§‹åŒ–æœ€ä½³å¥–åŠ±è®°å½• (ç”¨äºä¿å­˜æœ€ä½³æ¨¡å‹)
best_avg_reward = -np.inf 

# ==========================================
# è®­ç»ƒä¸»å¾ªç¯
# ==========================================
while timestep < max_timesteps:

    # å¦‚æœä¸€ä¸ª Episode ç»“æŸ
    if done:
        # åªè¦ä¸æ˜¯åˆšå¼€å§‹ï¼Œå°±è¿›è¡Œç½‘ç»œè®­ç»ƒ
        if timestep != 0:
            network.train(
                replay_buffer,
                episode_timesteps,
                batch_size,
                discount,
                tau,
                policy_noise,
                noise_clip,
                policy_freq,
            )

        # æ£€æŸ¥æ˜¯å¦éœ€è¦è¯„ä¼°
        if timesteps_since_eval >= eval_freq:
            print("Validating")
            timesteps_since_eval %= eval_freq

            # 1. è¯„ä¼°å½“å‰æ¨¡å‹
            avg_reward = evaluate(network=network, epoch=epoch, eval_episodes=eval_ep)
            evaluations.append(avg_reward)
            
            # 2. ä¿å­˜â€œæœ€æ–°â€æ¨¡å‹ (æ–¹ä¾¿æ–­ç‚¹ç»­ä¼ )
            network.save(file_name, directory="./pytorch_models")
            
            # 3. ä¿å­˜â€œæœ€ä¼˜â€æ¨¡å‹ (å¦‚æœå½“å‰åˆ†æ•°çªç ´å†å²æœ€é«˜)
            if avg_reward > best_avg_reward:
                best_avg_reward = avg_reward
                print(f"ğŸŒŸ New Best Model Found! Reward: {best_avg_reward:.2f} Saving...")
                network.save(file_name + "_best", directory="./pytorch_models")
            
            # 4. å®šæœŸå¤‡ä»½ (å¯é€‰)
            if epoch % 10 == 0:
                network.save(f"{file_name}_epoch_{epoch}", directory="./pytorch_models")

            np.save("./results/%s" % (file_name), evaluations)
            epoch += 1

        # é‡ç½®ç¯å¢ƒï¼Œå¼€å§‹æ–° Episode
        raw_state = env.reset()
        state = process_state(raw_state) # <--- å½’ä¸€åŒ–è¾“å…¥
        done = False

        episode_reward = 0
        episode_timesteps = 0
        episode_num += 1

    # --- æ¢ç´¢å™ªå£°è¡°å‡ ---
    if expl_noise > expl_min:
        expl_noise = expl_noise - ((1 - expl_min) / expl_decay_steps)

    # è·å–åŠ¨ä½œ
    action = network.get_action(np.array(state))
    # æ·»åŠ é«˜æ–¯å™ªå£°è¿›è¡Œæ¢ç´¢ (Exploration)
    action = (action + np.random.normal(0, expl_noise, size=action_dim)).clip(
        -max_action, max_action
    )

    # --- ç‰¹æ®Šæ¢å¤ç­–ç•¥: å¦‚æœç¦»éšœç¢ç‰©å¤ªè¿‘ï¼Œå¼ºåˆ¶æ‰§è¡ŒéšæœºåŠ¨ä½œ ---
    # ç›®çš„: é˜²æ­¢æœºå™¨äººé™·å…¥å±€éƒ¨æ­»èƒ¡åŒ
    if random_near_obstacle:
        if (
            np.random.uniform(0, 1) > 0.85 # æœ‰ä¸€å®šæ¦‚ç‡è§¦å‘
            # æ£€æŸ¥é›·è¾¾æœ€å°å€¼ï¼Œåˆ¤æ–­æ˜¯å¦è¿‡è¿‘ (æ³¨æ„è¿™é‡Œä½¿ç”¨å½’ä¸€åŒ–åçš„æ•°æ®æ¯”è¾ƒ)
            # SAFE_DIST(0.6) / LIDAR_MAX(10.0) = 0.06
            and min(state[0:environment_dim]) < SAFE_DIST / LIDAR_MAX 
            and count_rand_actions < 1
        ):
            count_rand_actions = np.random.randint(8, 15) # æŒç»­éšæœºåŠ¨ä½œ 8-15 æ­¥
            random_action = np.random.uniform(-1, 1, 2)

        if count_rand_actions > 0:
            count_rand_actions -= 1
            action = random_action
            action[0] = -1 # å¼ºåˆ¶å€’è½¦æˆ–å…¶å®ƒè¡Œä¸º

    # å°†åŠ¨ä½œè½¬æ¢ä¸ºç¯å¢ƒå¯æ¥å—çš„æ ¼å¼ (çº¿é€Ÿåº¦ [0,1], è§’é€Ÿåº¦ [-1,1])
    a_in = [(action[0] + 1) / 2, action[1]]
    
    # æ‰§è¡ŒåŠ¨ä½œ
    raw_next_state, reward, done, target = env.step(a_in)
    
    # å¤„ç† Next State (å½’ä¸€åŒ–)
    next_state = process_state(raw_next_state)

    # æ ‡è®°æ˜¯å¦å› ä¸ºè¾¾åˆ°æœ€å¤§æ­¥æ•°è€Œç»“æŸ (TimeLimit)
    # å¦‚æœæ˜¯å› ä¸ºæ­¥æ•°è€—å°½ï¼Œdone_bool åº”ä¸º 0 (ä»¥ä¾¿ Critic ä¾ç„¶è®¡ç®—åç»­ä»·å€¼)
    done_bool = 0 if episode_timesteps + 1 == max_ep else int(done)
    done = 1 if episode_timesteps + 1 == max_ep else int(done)
    episode_reward += reward

    # å­˜å…¥ Replay Buffer (å­˜å…¥çš„æ˜¯å½’ä¸€åŒ–åçš„ state)
    replay_buffer.add(state, action, reward, done_bool, next_state)

    state = next_state
    episode_timesteps += 1
    timestep += 1
    timesteps_since_eval += 1

# è®­ç»ƒç»“æŸåçš„æœ€ç»ˆä¿å­˜
evaluations.append(evaluate(network=network, epoch=epoch, eval_episodes=eval_ep))
if save_model:
    network.save("%s" % file_name, directory="./pytorch_models")
np.save("./results/%s" % file_name, evaluations)